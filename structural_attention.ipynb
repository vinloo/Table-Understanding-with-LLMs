{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b9aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "      \"Name\": [\"John\", \"Jane Lastname\"],\n",
    "        \"Age\": [30, 25],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2100c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name҈Age\n",
      "John҈30\n",
      "Jane Lastname҈25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "separator = '\\u0488'\n",
    "prompt_pre = \"This is a table\\n\\n\"\n",
    "prompt_table = df.to_csv(index=False, sep=separator)\n",
    "prompt_post = \"\\n\\nHow old is Jane?\\n\\nAnswer:\"\n",
    "\n",
    "print(prompt_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3da380",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_pre = \"\"\"You are a table analyst. Your task is to answer questions based on the table content.\n",
    "\n",
    "\n",
    "The answer should follow the format below:\n",
    "[Answer Format]\n",
    "Final Answer: AnswerName1, AnswerName2...\n",
    "\n",
    "Ensure the final answer format is the last output line and can only be in the \"Final Answer: AnswerName1, AnswerName2...\" form, no other form. Ensure the \"AnswerName\" is a number or entity name, as short as possible, without any explanation.\n",
    "\n",
    "Give the final answer to the question directly without any explanation.\n",
    "\n",
    "Read the table below:\n",
    "[TABLE]\n",
    "\"\"\"\n",
    "prompt_table = \"\"\"\n",
    "club\\u0009played\\u0009drawn\\u0009lost\\u0009points for\\u0009points against\\u0009points difference\\u0009tries for\\u0009tries against\\u0009try bonus\\u0009losing bonus\\u0009points\n",
    "club\\u0009played\\u0009drawn\\u0009lost\\u0009points for\\u0009points against\\u0009points difference\\u0009tries for\\u0009tries against\\u0009try bonus\\u0009losing bonus\\u0009points\n",
    "wattstown rfc\\u000916\\u00090\\u00090\\u0009361\\u0009117\\u0009+ 244\\u000939\\u000914\\u00095\\u00090\\u000969\n",
    "bryncethin rfc\\u000916\\u00090\\u00094\\u0009306\\u0009184\\u0009+ 122\\u000941\\u000926\\u00096\\u00092\\u000956\n",
    "crc caerdydd rfc\\u000916\\u00090\\u00095\\u0009280\\u0009197\\u0009+ 83\\u000939\\u000923\\u00094\\u00091\\u000949\n",
    "cambrian welfare rfc\\u000916\\u00091\\u00098\\u0009336\\u0009209\\u0009+ 127\\u000949\\u000920\\u00095\\u00096\\u000941\n",
    "glyncoch rfc\\u000916\\u00090\\u000910\\u0009206\\u0009248\\u0009- 42\\u000925\\u000931\\u00091\\u00096\\u000931\n",
    "llanrumney rfc\\u000916\\u00091\\u000910\\u0009277\\u0009304\\u0009- 27\\u000936\\u000938\\u00093\\u00093\\u000928\n",
    "ynysowen rfc\\u000916\\u00090\\u000911\\u0009240\\u0009339\\u0009- 99\\u000928\\u000949\\u00090\\u00093\\u000923\n",
    "caerau ely rfc\\u000916\\u00090\\u000912\\u0009163\\u0009273\\u0009- 110\\u000921\\u000933\\u00092\\u00094\\u000922\n",
    "llandrindod wells rfc\\u000916\\u00090\\u000911\\u0009155\\u0009453\\u0009- 298\\u000918\\u000962\\u00090\\u00091\\u000921\n",
    "\"\"\".lstrip()\n",
    "prompt_post = \"\"\"\n",
    "\n",
    "Let's get start!\n",
    "Question: How many points did Wattstown RFC score in the season?\n",
    "\n",
    "Final Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7061590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/miniconda3/envs/thesis/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8b\", revision=\"main\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deda926b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 2028, 374, 264, 2007, 271, 678, 142, 230, 17166, 198, 13379, 142, 230, 966, 198, 63602, 8155, 609, 142, 230, 914, 1432, 4438, 2362, 374, 22195, 1980, 16533, 25], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_pre + prompt_table + prompt_post\n",
    "tokenizer(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de13e2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 2028, 374, 264, 2007, 271, 678, 142, 230, 17166, 198, 13379, 142, 230, 966, 198, 63602, 8155, 609, 142, 230, 914, 1432, 4438, 2362, 374, 22195, 1980, 16533, 25]\n"
     ]
    }
   ],
   "source": [
    "tokens = [tokenizer.bos_token] + tokenizer.tokenize(prompt, return_tensors=\"pt\")\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa8aa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Ġis', 'Ġa', 'Ġtable', 'ĊĊ', 'Name', 'Ò', 'Ī', 'Age', 'Ċ', 'John', 'Ò', 'Ī', '30', 'Ċ', 'Jane', 'ĠLast', 'name', 'Ò', 'Ī', '25', 'Ċ', 'ĊĊ', 'How', 'Ġold', 'Ġis', 'ĠJane', '?ĊĊ', 'Answer', ':']\n",
      "[0, 1, 2, 3, 4, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "tokens_pre = tokenizer.tokenize(prompt_pre, return_tensors=\"pt\")\n",
    "tokens_table = tokenizer.tokenize(prompt_table, return_tensors=\"pt\")\n",
    "tokens_post = tokenizer.tokenize(prompt_post, return_tensors=\"pt\")\n",
    "tokens = tokens_pre + tokens_table + tokens_post\n",
    "\n",
    "text_ids = list(range(len(tokens_pre))) + list(range(len(tokens_pre) + len(tokens_table), len(tokens_pre) + len(tokens_table) + len(tokens_post)))\n",
    "table_ids = list(range(len(tokens_pre), len(tokens_pre) + len(tokens_table)))\n",
    "\n",
    "print(tokens)\n",
    "print(text_ids)\n",
    "print(table_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "133fe945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:\n",
      "\"Name\", \"Age\", \n",
      "\"John\", \"30\", \n",
      "\"JaneĠLastname\", \"25\", \n",
      "\n",
      "Cols:\n",
      "\"Name\", \"John\", \"JaneĠLastname\", \n",
      "\"Age\", \"30\", \"25\", \n",
      "\n",
      "[[[5], [8]], [[10], [13]], [[15, 16, 17], [20]], []]\n",
      "[[[5], [10], [15, 16, 17]], [[8], [13], [20]]]\n",
      "{5, 8, 10, 13, 15, 16, 17, 20}\n"
     ]
    }
   ],
   "source": [
    "curr_row, curr_col = 0, 0\n",
    "newline_tokens = tokenizer.tokenize(\"\\n\", return_tensors=\"pt\")\n",
    "separator_tokens = tokenizer.tokenize(separator, return_tensors=\"pt\")\n",
    "\n",
    "content_ids = set()\n",
    "rows_ids = []\n",
    "cols_ids = []\n",
    "just_added_col = False\n",
    "\n",
    "for token_id in table_ids:\n",
    "    if  tokens[token_id] in newline_tokens:\n",
    "        curr_row += 1\n",
    "        curr_col = 0\n",
    "\n",
    "    if len(rows_ids) <= curr_row:\n",
    "        rows_ids.append([])\n",
    "    if len(cols_ids) <= curr_col:\n",
    "        cols_ids.append([])\n",
    "    \n",
    "    if  tokens[token_id] in separator_tokens and not just_added_col:\n",
    "        curr_col += 1\n",
    "        just_added_col = True\n",
    "    else:\n",
    "        just_added_col = False\n",
    "\n",
    "    if tokens[token_id] not in separator_tokens and tokens[token_id] not in newline_tokens:\n",
    "        if len(rows_ids[curr_row]) <= curr_col:\n",
    "            rows_ids[curr_row].append([])\n",
    "        if len(cols_ids[curr_col]) <= curr_row:\n",
    "            cols_ids[curr_col].append([])\n",
    "\n",
    "        rows_ids[curr_row][curr_col].append(token_id)\n",
    "        cols_ids[curr_col][curr_row].append(token_id)\n",
    "        content_ids.add(token_id)\n",
    "\n",
    "\n",
    "print(\"Rows:\")\n",
    "for row in rows_ids:\n",
    "    for cell in row:\n",
    "        print('\"', end='')\n",
    "        for token_id in cell:\n",
    "            print(tokens[token_id], end='')\n",
    "        print('\",', end=' ')\n",
    "    print()\n",
    "\n",
    "print(\"Cols:\")\n",
    "for col in cols_ids:\n",
    "    for cell in col:\n",
    "        print('\"', end='')\n",
    "        for token_id in cell:\n",
    "            print(tokens[token_id], end='')\n",
    "        print('\",', end=' ')\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print(rows_ids)\n",
    "print(cols_ids)\n",
    "print(content_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f7933ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(5, 8),\n",
       " (5, 10),\n",
       " (5, 15),\n",
       " (5, 16),\n",
       " (5, 17),\n",
       " (8, 13),\n",
       " (8, 20),\n",
       " (10, 13),\n",
       " (10, 15),\n",
       " (10, 16),\n",
       " (10, 17),\n",
       " (13, 20),\n",
       " (15, 16),\n",
       " (15, 17),\n",
       " (15, 20),\n",
       " (16, 17),\n",
       " (16, 20),\n",
       " (17, 20)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_pairs = set()\n",
    "\n",
    "for row in rows_ids:\n",
    "    for i, cell in enumerate(row):\n",
    "        for ti, token_id in enumerate(cell):\n",
    "\n",
    "            for token_id2 in cell[ti + 1:]:\n",
    "                attention_pairs.add((token_id, token_id2))\n",
    "\n",
    "            for cell2 in row[i + 1:]:\n",
    "                for token_id2 in cell2:\n",
    "                    attention_pairs.add((token_id, token_id2))\n",
    "\n",
    "for col in cols_ids:\n",
    "    for i, cell in enumerate(col):\n",
    "        for token_id in cell:\n",
    "            for cell2 in col[i + 1:]:\n",
    "                for token_id2 in cell2:\n",
    "                    attention_pairs.add((token_id, token_id2))\n",
    "\n",
    "attention_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058bf9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8b\", trust_remote_code=True)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config._attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5145e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): TabularLlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Callable, Optional, Tuple, Union, Unpack\n",
    "from transformers.utils import logging\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "def eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    scaling: float,\n",
    "    dropout: float = 0.0,\n",
    "    **kwargs,\n",
    "):\n",
    "    key_states = repeat_kv(key, module.num_key_value_groups)\n",
    "    value_states = repeat_kv(value, module.num_key_value_groups)\n",
    "\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    if attn_weights.shape[2] == attn_weights.shape[3] == len(tokens):\n",
    "        for i in table_ids:\n",
    "            for j in table_ids:\n",
    "                if i != j:\n",
    "                    if (i,j) not in attention_pairs and (j,i) not in attention_pairs:\n",
    "                        attn_weights[:, :, i, j] = float(-1e9)\n",
    "                        attn_weights[:, :, j, i] = float(-1e9)\n",
    "\n",
    "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
    "    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n",
    "\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    return attn_output, attn_weights\n",
    "\n",
    "class TabularLlamaAttention(LlamaAttention):\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        attn_output, attn_weights = eager_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "for i, layer in enumerate(model.model.layers):\n",
    "    original_attn = layer.self_attn\n",
    "    tabular_attn = TabularLlamaAttention(model.config, i)\n",
    "    tabular_attn.load_state_dict(original_attn.state_dict())\n",
    "    layer.self_attn = tabular_attn\n",
    "\n",
    "\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "205dfce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is a table\\n\\nName҈Age\\nJohn҈30\\nJane Lastname҈25\\n\\n\\nHow old is Jane?\\n\\nAnswer: 25\\n\\nJane is 25 years old.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.convert_tokens_to_ids(tokens)\n",
    "inputs = torch.tensor([inputs]).to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = model.generate(input_ids=inputs)\n",
    "\n",
    "tokenizer.decode(result[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
